{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementando RAG \n",
    "ChromaDB, Langchain e ChatGPT API\n",
    "\n",
    "> Based on: https://medium.com/@drjulija/how-i-built-a-basic-rag-for-pdf-qa-in-a-few-lines-of-python-code-9849c32e59f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setar a variável de ambiente antes abrir o vscode\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') # add your OpenAI API Key\n",
    "DOC_PATH = \"./data/doc.pdf\"\n",
    "CHROMA_PATH = \"./db\" # ou user instalação do chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Data Indexing Process -----\n",
    "\n",
    "# load your pdf doc\n",
    "loader = PyPDFLoader(DOC_PATH)\n",
    "pages = loader.load()\n",
    "\n",
    "# split the doc into smaller chunks i.e. chunk_size=500\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# get OpenAI Embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# embed the chunks as vectors and load them into the database\n",
    "db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Retrieval and Generation Process -----\n",
    "\n",
    "# this is an example of a user question (query)\n",
    "query = 'what is this document about?'\n",
    "\n",
    "# retrieve context - top 5 most relevant (closests) chunks to the query vector \n",
    "# (by default Langchain is using cosine distance metric)\n",
    "docs_chroma = db_chroma.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# generate an answer based on given user query and retrieved context information\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
    "\n",
    "# you can use a prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "Answer the question based on the above context: {question}.\n",
    "Provide a short answer.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n",
    "\n",
    "# load retrieved context and user query in the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "# call LLM model to generate the answer based on the given context and query\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7\n",
    "# response_text = model.predict(prompt) \n",
    "# ver mais detalhes em https://python.langchain.com/v0.2/docs/versions/v0_2/deprecations/\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "response_text = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baby names, specifically the name Caio, its origin, meaning, and popularity.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
