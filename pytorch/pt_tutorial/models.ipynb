{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(100, 200)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200, 10)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "tiny_model = TinyModel()\n",
    "print(tiny_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0599,  0.0374, -0.0713,  ...,  0.0261,  0.0272, -0.0440],\n",
      "        [ 0.0913, -0.0499, -0.0633,  ...,  0.0938,  0.0671,  0.0498],\n",
      "        [ 0.0488,  0.0582, -0.0928,  ..., -0.0435,  0.0292,  0.0893],\n",
      "        ...,\n",
      "        [-0.0525,  0.0408, -0.0647,  ..., -0.0447,  0.0820, -0.0416],\n",
      "        [-0.0350,  0.0921, -0.0649,  ..., -0.0439, -0.0367,  0.0733],\n",
      "        [-0.0182,  0.0480,  0.0634,  ..., -0.0078,  0.0457,  0.0339]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0111, -0.0105, -0.0015,  0.0892, -0.0154,  0.0468,  0.0806,  0.0600,\n",
      "         0.0237, -0.0015, -0.0477,  0.0290, -0.0930, -0.0838,  0.0960,  0.0604,\n",
      "        -0.0168,  0.0245, -0.0535,  0.0293, -0.0887, -0.0345, -0.0085,  0.0187,\n",
      "        -0.0867, -0.0839,  0.0854, -0.0098, -0.0657, -0.0127, -0.0357,  0.0146,\n",
      "        -0.0065,  0.0254,  0.0366, -0.0237,  0.0551,  0.0848, -0.0648,  0.0849,\n",
      "        -0.0439, -0.0444, -0.0567, -0.0409,  0.0652,  0.0017, -0.0595, -0.0114,\n",
      "         0.0986, -0.0198, -0.0914,  0.0610, -0.0635,  0.0443,  0.0483, -0.0621,\n",
      "        -0.0903, -0.0451, -0.0728,  0.0112, -0.0965,  0.0676,  0.0044,  0.0169,\n",
      "         0.0755, -0.0784,  0.0952, -0.0894, -0.0067, -0.0439,  0.0760, -0.0396,\n",
      "        -0.0004,  0.0103, -0.0840, -0.0880,  0.0069,  0.0546,  0.0618,  0.0524,\n",
      "        -0.0557,  0.0269,  0.0989, -0.0816, -0.0296, -0.0811, -0.0327, -0.0072,\n",
      "        -0.0176, -0.0022, -0.0372,  0.0274, -0.0360, -0.0019, -0.0952, -0.0187,\n",
      "         0.0570, -0.0966, -0.0861, -0.0960, -0.0528, -0.0108,  0.0645, -0.0344,\n",
      "        -0.0843,  0.0396, -0.0392,  0.0851, -0.0368, -0.0067, -0.0479, -0.0988,\n",
      "        -0.0796,  0.0444, -0.0594, -0.0201,  0.0125,  0.0548, -0.0926, -0.0720,\n",
      "        -0.0188,  0.0883,  0.0485, -0.0542, -0.0656,  0.0708,  0.0297,  0.0341,\n",
      "         0.0164,  0.0640,  0.0652, -0.0742,  0.0267,  0.0946, -0.0243,  0.0063,\n",
      "        -0.0306, -0.0911,  0.0606,  0.0884, -0.0874, -0.0203, -0.0495, -0.0617,\n",
      "         0.0462, -0.0267, -0.0905,  0.0854,  0.0092,  0.0666,  0.0952,  0.0051,\n",
      "        -0.0627, -0.0449, -0.0361,  0.0690, -0.0163, -0.0214,  0.0656, -0.0692,\n",
      "         0.0420,  0.0685,  0.0133, -0.0335,  0.0544,  0.0590, -0.0602,  0.0326,\n",
      "        -0.0265,  0.0203,  0.0776,  0.0750,  0.0143,  0.0956,  0.0408,  0.0876,\n",
      "         0.0250, -0.0885, -0.0466, -0.0529,  0.0804, -0.0532, -0.0849,  0.0944,\n",
      "        -0.0864, -0.0591, -0.0146, -0.0001, -0.0767,  0.0585, -0.0398, -0.0473,\n",
      "        -0.0673, -0.0281,  0.0815,  0.0277, -0.0595,  0.0162, -0.0801,  0.0147],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.1201e-02,  1.4560e-02, -3.5855e-02,  ...,  5.9941e-02,\n",
      "          4.7385e-02, -7.1248e-03],\n",
      "        [ 6.0764e-02,  4.7733e-02, -5.9405e-02,  ...,  5.8891e-02,\n",
      "          1.2440e-02,  2.2069e-02],\n",
      "        [-2.1890e-02, -4.2067e-02,  6.8043e-02,  ..., -5.9911e-02,\n",
      "          4.6748e-02,  3.6947e-02],\n",
      "        ...,\n",
      "        [-5.9539e-02,  3.9088e-02,  1.3436e-02,  ..., -2.5686e-02,\n",
      "          2.8275e-02,  1.3285e-02],\n",
      "        [ 2.9305e-02,  2.5379e-02, -3.1464e-02,  ..., -9.2827e-03,\n",
      "         -2.0208e-02,  3.6044e-02],\n",
      "        [ 5.2464e-05,  9.3200e-03, -1.5876e-02,  ...,  6.7781e-02,\n",
      "         -1.6186e-02,  3.9045e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0089, -0.0421, -0.0142,  0.0374, -0.0515, -0.0011, -0.0024,  0.0452,\n",
      "        -0.0003, -0.0187], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('The model:')\n",
    "print(tiny_model)\n",
    "\n",
    "print('Model params:')\n",
    "for param in tiny_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-1.1201e-02,  1.4560e-02, -3.5855e-02,  ...,  5.9941e-02,\n",
      "          4.7385e-02, -7.1248e-03],\n",
      "        [ 6.0764e-02,  4.7733e-02, -5.9405e-02,  ...,  5.8891e-02,\n",
      "          1.2440e-02,  2.2069e-02],\n",
      "        [-2.1890e-02, -4.2067e-02,  6.8043e-02,  ..., -5.9911e-02,\n",
      "          4.6748e-02,  3.6947e-02],\n",
      "        ...,\n",
      "        [-5.9539e-02,  3.9088e-02,  1.3436e-02,  ..., -2.5686e-02,\n",
      "          2.8275e-02,  1.3285e-02],\n",
      "        [ 2.9305e-02,  2.5379e-02, -3.1464e-02,  ..., -9.2827e-03,\n",
      "         -2.0208e-02,  3.6044e-02],\n",
      "        [ 5.2464e-05,  9.3200e-03, -1.5876e-02,  ...,  6.7781e-02,\n",
      "         -1.6186e-02,  3.9045e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0089, -0.0421, -0.0142,  0.0374, -0.0515, -0.0011, -0.0024,  0.0452,\n",
      "        -0.0003, -0.0187], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('One Layer:')\n",
    "print(tiny_model.linear2)\n",
    "\n",
    "print('Layer params:')\n",
    "for param in tiny_model.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Layer Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[0.6943, 0.5076, 0.7240]])\n",
      "-------------------------------------------------------\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.2247,  0.5034, -0.3776],\n",
      "        [-0.3783, -0.0466,  0.0951]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5374, -0.4108], requires_grad=True)\n",
      "-------------------------------------------------------\n",
      "Output: tensor([[-0.7113, -0.6283]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "\n",
    "lin = nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:', x)\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "print('Weight and Bias parameters:')\n",
    "for params in lin.parameters():\n",
    "    print(params)\n",
    "\n",
    "y = lin(x)\n",
    "print('-------------------------------------------------------')\n",
    "print('Output:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional\n",
    "\n",
    "import torch.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(LeNet, self).__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = x.view(-1, 576) # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "conv_model = LeNet()\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Layers\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(10000, 100)\n",
      "  (lstm): LSTM(100, 50)\n",
      "  (hidden2tag): Linear(in_features=50, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMTagger(100, 50, 10000, 50)\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiagoads/Workspace/Programas/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "tensor([[[-1.1356,  0.3706, -0.5995,  ...,  1.9961, -0.7448,  0.6982],\n",
      "         [-0.4686,  1.1001, -0.6223,  ...,  1.3844, -0.6552,  1.1214],\n",
      "         [-0.4276,  0.7383, -0.2367,  ...,  1.1515, -0.3914,  0.9696],\n",
      "         ...,\n",
      "         [-1.7485,  1.7391, -0.5444,  ...,  1.2300, -0.8015,  1.1522],\n",
      "         [-1.2952,  0.0982, -0.2604,  ...,  1.4607, -1.5242,  1.0012],\n",
      "         [-1.6317,  0.3913,  0.1359,  ...,  1.0271, -0.6552,  1.2051]],\n",
      "\n",
      "        [[-1.3488,  0.1134, -1.0839,  ...,  0.9069, -0.4907,  0.1890],\n",
      "         [-0.8734,  0.4317, -1.2442,  ...,  1.0628, -0.7078,  1.5002],\n",
      "         [-0.4615,  0.0963, -0.3790,  ...,  0.4699, -0.5530,  0.8179],\n",
      "         ...,\n",
      "         [-2.2628,  1.0143, -0.7417,  ...,  0.9421, -0.8136,  1.4209],\n",
      "         [ 0.1062,  0.5913, -0.0270,  ...,  0.7620, -0.2963,  1.7456],\n",
      "         [-1.2824,  0.6095,  0.9510,  ...,  1.5390, -0.9210,  1.5832]],\n",
      "\n",
      "        [[-1.3235,  0.3681, -0.2085,  ...,  1.2336, -1.3569,  0.5902],\n",
      "         [-0.2127,  0.3746, -1.0833,  ...,  1.6123, -0.9196,  1.8493],\n",
      "         [-0.5923,  1.0399, -1.0469,  ...,  0.5122, -0.9895,  0.4663],\n",
      "         ...,\n",
      "         [-2.8004,  2.1186, -0.1492,  ...,  1.9050, -1.0393, -0.2783],\n",
      "         [-1.1524,  0.6945, -0.3923,  ...,  0.9838, -0.8898,  1.6295],\n",
      "         [-2.2690,  0.2061, -0.5791,  ...,  1.5075, -0.7597,  0.2783]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4498, -0.2226, -0.5320,  ...,  1.4414, -0.0784,  0.6395],\n",
      "         [-1.0468,  0.3141, -0.9807,  ...,  0.6798, -0.7391,  1.9679],\n",
      "         [-0.3040,  0.7239, -0.0823,  ...,  0.8363, -0.3407,  1.1706],\n",
      "         ...,\n",
      "         [-1.4704,  0.5382, -0.1061,  ...,  1.3993, -0.7803,  1.4600],\n",
      "         [-0.3073,  0.6613, -0.8652,  ...,  0.9121, -0.0480,  2.4586],\n",
      "         [-2.4340, -0.3635, -0.2177,  ...,  1.6746, -0.7304,  2.1798]],\n",
      "\n",
      "        [[-1.2803,  0.8704, -0.2463,  ...,  1.0811, -0.7884,  0.9910],\n",
      "         [-0.8458,  0.3579, -0.9272,  ...,  1.6722, -0.3913,  1.3633],\n",
      "         [-0.4754,  0.4344,  0.1105,  ...,  1.1199,  0.0649,  1.4476],\n",
      "         ...,\n",
      "         [-1.4606,  1.9025, -0.4065,  ...,  1.3853, -0.1146,  1.7231],\n",
      "         [ 0.0173,  0.4708, -0.4484,  ...,  0.9309, -0.8897,  1.6966],\n",
      "         [-2.0410,  0.2084, -1.1281,  ...,  1.4544, -0.2139,  2.1221]],\n",
      "\n",
      "        [[-1.1529,  0.3598, -0.9855,  ...,  1.9839, -0.9748,  0.6521],\n",
      "         [-0.1622, -0.0372, -0.7427,  ...,  0.7157, -0.6132,  1.6702],\n",
      "         [ 0.3207,  1.0090, -0.1843,  ...,  0.6210, -0.7770,  0.7674],\n",
      "         ...,\n",
      "         [-2.2068,  1.6297, -0.3549,  ...,  2.1170, -0.2531,  1.6142],\n",
      "         [-0.2601,  0.3665, -0.1208,  ...,  0.9913, -1.1791,  1.5529],\n",
      "         [-0.9968,  0.1565, -0.1529,  ...,  1.6057, -0.8789,  1.4492]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Transformers\n",
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "input = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "output = transformer_model(input, tgt)\n",
    "print(transformer_model)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[[ 0.4952, -1.7868, -0.1444,  ..., -0.7066,  1.3589, -0.7895],\n",
      "         [ 0.9629, -1.3782,  0.9430,  ...,  1.2434,  0.6322, -0.1833],\n",
      "         [ 1.9673, -0.5689,  0.3360,  ...,  0.9851,  0.9429, -0.0715],\n",
      "         ...,\n",
      "         [-0.1213, -0.2957,  0.7424,  ...,  0.3319,  0.0518, -0.9152],\n",
      "         [ 0.7456, -0.9266,  0.8963,  ...,  0.2022,  0.5455, -0.8276],\n",
      "         [ 0.5741, -1.9751,  0.9001,  ..., -0.2822,  0.5856, -1.0285]],\n",
      "\n",
      "        [[ 1.1353, -0.3436, -0.0153,  ...,  0.0279,  1.5634, -0.9388],\n",
      "         [ 0.3688, -1.1751,  0.2934,  ..., -0.0458, -0.3559, -0.5056],\n",
      "         [ 1.2466, -1.7587, -0.5380,  ...,  0.4844,  0.7863,  0.6405],\n",
      "         ...,\n",
      "         [ 1.2607, -0.0236,  0.3749,  ...,  0.1265, -0.2961, -0.2244],\n",
      "         [ 1.3189, -0.8835,  0.4749,  ...,  0.9118,  0.4607,  0.3159],\n",
      "         [ 1.4942, -1.8175,  0.8095,  ..., -0.3655,  1.2463, -0.5019]],\n",
      "\n",
      "        [[ 0.9274, -0.6656,  0.0320,  ...,  0.1302,  1.0764, -0.3428],\n",
      "         [ 1.1139, -0.6753, -0.1648,  ...,  1.0253,  0.1042, -1.2945],\n",
      "         [ 1.3312, -0.3599, -0.1286,  ...,  1.0019,  1.0081,  0.0492],\n",
      "         ...,\n",
      "         [ 0.9616, -1.4062,  0.8395,  ..., -0.1925,  0.2730, -1.2978],\n",
      "         [-0.3486, -1.0133,  0.5775,  ..., -0.2581, -0.3223, -0.1140],\n",
      "         [ 1.4109, -2.0747,  0.2143,  ..., -0.6266,  0.9381, -0.1864]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2896, -1.3023,  0.0541,  ...,  0.4247,  0.6308, -0.9114],\n",
      "         [ 1.3734, -0.4410,  0.0361,  ...,  0.5037, -0.0322, -0.1827],\n",
      "         [ 1.3305, -1.4929, -0.1071,  ...,  0.1132,  0.3387, -0.1625],\n",
      "         ...,\n",
      "         [ 0.8341, -0.5314, -0.4589,  ..., -0.3425,  0.2205, -0.8741],\n",
      "         [ 0.8092, -1.0214,  0.9040,  ..., -0.3619,  0.8563, -0.5036],\n",
      "         [ 0.7909, -2.2464, -0.0911,  ..., -0.4215,  1.3654, -0.0560]],\n",
      "\n",
      "        [[ 1.0567, -0.8277,  0.1031,  ...,  0.1658,  0.7815, -1.1160],\n",
      "         [ 1.3634, -1.5348,  0.0262,  ...,  0.6529,  0.3666, -0.0503],\n",
      "         [ 1.2245, -1.0425,  0.7480,  ...,  1.2859,  0.6130,  0.5410],\n",
      "         ...,\n",
      "         [ 1.0054, -0.6615,  1.2338,  ...,  0.3065,  0.1887, -0.8125],\n",
      "         [ 0.2266, -0.1954,  0.6749,  ...,  0.5237,  0.4737, -0.0215],\n",
      "         [ 0.7356, -2.3088,  0.0066,  ...,  0.1891,  0.7859,  0.2519]],\n",
      "\n",
      "        [[ 1.3260, -1.1096,  0.6279,  ..., -0.1282,  0.3388, -0.7147],\n",
      "         [ 0.6327, -1.3998,  0.4984,  ..., -0.0480,  0.2512,  0.2380],\n",
      "         [ 1.5634, -0.6484, -0.7258,  ...,  0.6682,  0.7354,  0.6513],\n",
      "         ...,\n",
      "         [ 0.5577, -0.5871,  0.5539,  ...,  0.5215,  0.6063, -1.4598],\n",
      "         [ 0.6600, -0.7050,  0.9434,  ..., -0.2689,  1.0002, -0.6494],\n",
      "         [ 1.2363, -0.9574, -0.0169,  ..., -0.1949,  0.1864,  0.1132]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiagoads/Workspace/Programas/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "input = torch.rand(10, 32, 512)\n",
    "output = transformer_encoder(input)\n",
    "print(transformer_encoder)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[[-4.0713e-01, -3.4067e-02,  3.1157e-01,  ...,  2.0228e+00,\n",
      "          -5.8321e-01,  8.7913e-01],\n",
      "         [-6.6332e-01, -5.0576e-01,  3.2460e-01,  ...,  2.2201e+00,\n",
      "          -6.4386e-01,  9.3610e-01],\n",
      "         [-4.9623e-01, -6.8572e-01,  1.3749e-01,  ...,  2.6179e+00,\n",
      "          -5.6377e-01,  3.3156e-01],\n",
      "         ...,\n",
      "         [-6.1797e-02, -1.4703e-01, -5.7918e-01,  ...,  2.1052e+00,\n",
      "          -1.1440e+00,  9.2714e-01],\n",
      "         [-4.4450e-01, -2.0437e-01,  2.9727e-01,  ...,  2.0955e+00,\n",
      "          -1.0830e+00,  5.7292e-01],\n",
      "         [-2.6224e-01, -1.0225e-01,  8.3716e-01,  ...,  2.6061e+00,\n",
      "          -6.9598e-01,  1.6264e-01]],\n",
      "\n",
      "        [[-1.3234e-02, -6.3021e-01,  3.8379e-01,  ...,  2.2078e+00,\n",
      "          -6.8791e-01, -1.1907e-02],\n",
      "         [-1.4767e-01, -9.2413e-01, -1.8784e-02,  ...,  2.2424e+00,\n",
      "          -7.1204e-01,  6.1157e-01],\n",
      "         [-2.9014e-01, -6.1083e-01,  4.5234e-01,  ...,  1.2048e+00,\n",
      "          -2.7056e-01,  4.5106e-01],\n",
      "         ...,\n",
      "         [ 5.3546e-02, -9.5062e-02, -3.6947e-01,  ...,  1.7286e+00,\n",
      "          -1.1346e+00,  7.3048e-01],\n",
      "         [-2.9683e-01, -1.3089e-01,  9.2572e-01,  ...,  2.5570e+00,\n",
      "          -8.0756e-01,  6.7681e-01],\n",
      "         [-1.2804e-01, -2.6324e-01,  3.6397e-02,  ...,  2.7280e+00,\n",
      "          -7.4509e-01, -2.5925e-01]],\n",
      "\n",
      "        [[-2.7162e-01, -3.0315e-01,  5.7734e-01,  ...,  2.2826e+00,\n",
      "          -1.6719e-01,  8.0278e-01],\n",
      "         [-4.0550e-01, -4.7214e-01,  4.2474e-01,  ...,  2.7593e+00,\n",
      "          -1.1423e+00,  4.2536e-01],\n",
      "         [ 5.2691e-01, -6.9940e-01,  9.4568e-01,  ...,  2.9628e+00,\n",
      "          -1.1295e+00,  1.1648e-01],\n",
      "         ...,\n",
      "         [-3.6908e-01, -2.0340e-01, -4.4889e-01,  ...,  1.9235e+00,\n",
      "          -8.0834e-01,  7.2733e-01],\n",
      "         [ 1.7271e-01, -5.5399e-01,  2.4401e-01,  ...,  2.2059e+00,\n",
      "          -1.2317e+00,  4.7966e-01],\n",
      "         [ 1.3362e-01,  1.4660e-01,  5.7109e-01,  ...,  3.0598e+00,\n",
      "          -8.3037e-01,  7.7338e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.5080e-01, -5.5249e-01,  1.9758e-01,  ...,  2.4293e+00,\n",
      "          -5.2874e-01,  8.2978e-01],\n",
      "         [-3.9750e-02, -8.4801e-01,  7.2144e-01,  ...,  2.5395e+00,\n",
      "          -9.4701e-01,  6.5115e-01],\n",
      "         [-9.7706e-02,  1.5898e-01,  8.3157e-01,  ...,  2.5314e+00,\n",
      "          -7.7727e-01,  1.2481e-01],\n",
      "         ...,\n",
      "         [-1.9242e-01,  3.4003e-01,  7.7989e-03,  ...,  1.7628e+00,\n",
      "          -1.1279e+00,  5.2510e-01],\n",
      "         [ 7.5039e-01, -1.8488e-01,  1.1191e+00,  ...,  2.3087e+00,\n",
      "          -1.3090e+00,  3.1614e-01],\n",
      "         [-1.2932e-01,  3.1586e-01,  2.9801e-01,  ...,  2.5850e+00,\n",
      "          -9.2208e-01,  6.3901e-01]],\n",
      "\n",
      "        [[-2.3885e-01, -4.9104e-01,  7.4738e-02,  ...,  1.5793e+00,\n",
      "          -3.6499e-01,  7.2333e-01],\n",
      "         [-2.8353e-01, -2.2169e-01,  3.6036e-01,  ...,  2.5811e+00,\n",
      "          -1.1576e+00,  5.3808e-01],\n",
      "         [-1.3511e-01,  1.5803e-01,  3.1992e-01,  ...,  2.9353e+00,\n",
      "          -5.3754e-01,  3.3294e-01],\n",
      "         ...,\n",
      "         [ 6.0583e-02,  3.6826e-01,  1.2316e-01,  ...,  1.5943e+00,\n",
      "          -1.0882e+00,  7.7525e-01],\n",
      "         [ 7.4259e-01, -3.8171e-01,  1.0328e-03,  ...,  2.1227e+00,\n",
      "          -9.8117e-01,  4.4611e-01],\n",
      "         [-2.7435e-01, -2.7301e-01,  5.0152e-01,  ...,  3.0889e+00,\n",
      "          -1.3314e+00,  2.2752e-01]],\n",
      "\n",
      "        [[-9.9015e-02, -7.3528e-01,  3.8467e-01,  ...,  2.2200e+00,\n",
      "          -1.2103e-01,  6.0061e-01],\n",
      "         [ 1.2411e-02, -8.2040e-01, -4.0527e-01,  ...,  2.0515e+00,\n",
      "          -7.1291e-01,  6.0895e-02],\n",
      "         [-3.8575e-01, -4.6674e-01,  6.0560e-01,  ...,  2.4600e+00,\n",
      "          -7.2301e-01,  5.3613e-01],\n",
      "         ...,\n",
      "         [-2.0204e-01, -1.3397e-01, -4.0096e-01,  ...,  1.1791e+00,\n",
      "          -1.0109e+00,  8.9029e-01],\n",
      "         [-2.1006e-01, -4.5144e-01, -1.3403e-01,  ...,  2.2494e+00,\n",
      "          -1.0873e+00,  3.0491e-02],\n",
      "         [-2.5873e-01,  1.3515e-01,  5.2651e-01,  ...,  2.2224e+00,\n",
      "          -8.4680e-01,  1.1722e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "print(transformer_decoder)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2084, 0.9683, 0.4169, 0.8430, 0.9849, 0.1134],\n",
      "         [0.2743, 0.7948, 0.3798, 0.9022, 0.2047, 0.0855],\n",
      "         [0.1745, 0.9958, 0.1633, 0.5394, 0.7763, 0.8241],\n",
      "         [0.4677, 0.6048, 0.7953, 0.8265, 0.3833, 0.5445],\n",
      "         [0.5640, 0.1609, 0.1808, 0.5632, 0.9814, 0.4485],\n",
      "         [0.6967, 0.9981, 0.8327, 0.2695, 0.3640, 0.6561]]])\n",
      "tensor([[[0.9958, 0.9849],\n",
      "         [0.9981, 0.9814]]])\n"
     ]
    }
   ],
   "source": [
    "# Other Layers and Functions\n",
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[22.0691, 22.9752, 10.7985,  9.9651],\n",
      "         [ 7.6940,  9.0773, 12.1485, 15.4833],\n",
      "         [11.3351, 18.9554,  7.1318, 19.9361],\n",
      "         [18.9342,  5.9508, 11.4436, 19.5958]]])\n",
      "tensor(13.9684)\n",
      "tensor([[[ 0.9230,  1.0719, -0.9290, -1.0659],\n",
      "         [-1.1355, -0.6745,  0.3492,  1.4607],\n",
      "         [-0.5638,  0.8661, -1.3525,  1.0501],\n",
      "         [ 0.8791, -1.4252, -0.4504,  0.9965]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(-4.4703e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8128, 0.6534, 0.2062, 0.6433],\n",
      "         [0.6296, 0.4932, 0.5928, 0.6854],\n",
      "         [0.5585, 0.8713, 0.1364, 0.9941],\n",
      "         [0.9421, 0.9414, 0.0943, 0.6333]]])\n",
      "tensor([[[1.3547, 1.0889, 0.3437, 1.0721],\n",
      "         [1.0493, 0.0000, 0.9880, 0.0000],\n",
      "         [0.9308, 0.0000, 0.2274, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0556]]])\n",
      "tensor([[[0.0000, 1.0889, 0.3437, 1.0721],\n",
      "         [1.0493, 0.0000, 0.0000, 1.1423],\n",
      "         [0.9308, 1.4522, 0.0000, 1.6568],\n",
      "         [1.5701, 0.0000, 0.1572, 1.0556]]])\n",
      "tensor([[[0.0000, 0.0000, 0.3437, 1.0721],\n",
      "         [0.0000, 0.8220, 0.9880, 1.1423],\n",
      "         [0.9308, 0.0000, 0.2274, 0.0000],\n",
      "         [1.5701, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "print(my_tensor)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[[-1.3407,  1.3262]]])\n",
      "------------------------------------\n",
      "Relu:  tensor([[[0.0000, 1.3262]]])\n",
      "Sigmoid:  tensor([[[0.2074, 0.7902]]])\n",
      "Softmax: tensor([[[1., 1.]]])\n",
      "Tanh: tensor([[[-0.8718,  0.8683]]])\n",
      "------------------------------------\n",
      "Relu: tensor([[[0.0000, 1.3262]]])\n",
      "Sigmoid: tensor([[[0.2074, 0.7902]]])\n",
      "Softmax: tensor([[[1., 1.]]])\n",
      "Tanh: tensor([[[-0.8718,  0.8683]]])\n",
      "------------------------------------\n",
      "L1:  tensor(0.3102)\n",
      "MSE:  tensor(0.1275)\n",
      "CE: tensor(1.2655)\n",
      "BCE: tensor(0.7424)\n",
      "------------------------------------\n",
      "L1:  tensor(0.3102)\n",
      "MSE:  tensor(0.1275)\n",
      "CE: tensor(1.2655)\n",
      "BCE: tensor(0.7424)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 2)\n",
    "print('Input: ', input)\n",
    "print('------------------------------------')\n",
    "# activation functions\n",
    "print('Relu: ', torch.nn.ReLU()(input))\n",
    "print('Sigmoid: ', torch.nn.Sigmoid()(input))\n",
    "print('Softmax:', torch.nn.Softmax(dim=1)(input))\n",
    "print('Tanh:', torch.nn.Tanh()(input))\n",
    "print('------------------------------------')\n",
    "print('Relu:', torch.nn.functional.relu(input))\n",
    "print('Sigmoid:', torch.nn.functional.sigmoid(input))\n",
    "print('Softmax:', torch.nn.functional.softmax(input, dim=1))\n",
    "print('Tanh:', torch.nn.functional.tanh(input))\n",
    "\n",
    "\n",
    "h = torch.rand(1, 2)\n",
    "y = torch.rand(1, 2)\n",
    "\n",
    "# loss function\n",
    "print('------------------------------------')\n",
    "print('L1: ', torch.nn.functional.l1_loss(input=h, target=y))\n",
    "print('MSE: ', torch.nn.functional.mse_loss(input=h, target=y))\n",
    "print('CE:', torch.nn.functional.cross_entropy(input=h, target=y))\n",
    "print('BCE:', torch.nn.functional.binary_cross_entropy(input=h, target=y))\n",
    "print('------------------------------------')\n",
    "print('L1: ', torch.nn.L1Loss()(h, y))\n",
    "print('MSE: ', torch.nn.MSELoss()(h, y))\n",
    "print('CE:', torch.nn.CrossEntropyLoss()(h, y))\n",
    "print('BCE:', torch.nn.BCELoss()(h, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
